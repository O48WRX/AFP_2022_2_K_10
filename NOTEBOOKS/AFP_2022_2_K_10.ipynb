{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AFP_2022_2_K_10.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "># AFP_2022_2_K_10_Akropolisz Gépi tanulás framework\n",
        "---\n",
        "Hadobás Dávid (TB3376), Kardos Zsolt (O48WRX), Riczkó Henrik (D5GPJ6), Balogh Mihály Viktor (GUFVXA)\n",
        "---"
      ],
      "metadata": {
        "id": "AfNuiLPnSYmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, numpy, random\n",
        "adatbazis_helye = \"xd\" #@param {type:\"string\"}\n",
        "#@markdown --- \n",
        "algoritmus = 'C' #@param [\"A\", \"B\", \"C\"]\n",
        "aktivaciosfgv = 'sigmoid' #@param ['tanh', 'sigmoid']\n",
        "print('Választott algoritmus: ', algoritmus)\n",
        "print('Választott aktivációs függvény: ', aktivaciosfgv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-t_tKyeA1ac",
        "outputId": "6c337a3d-56a8-4b99-8406-bbbb35731086"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Választott algoritmus:  C\n",
            "Választott aktivációs függvény:  sigmoid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jelen állapotban az első kódblokkot a drop-down menük opciói kiválasztása után újra kell futtatni!!!!"
      ],
      "metadata": {
        "id": "FSAW3A6LasrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if aktivaciosfgv == 'tanh':\n",
        "  def activation(x):\n",
        "    return numpy.tanh(x)\n",
        "  def dactivation(x):\n",
        "    return 1.0 - x**2\n",
        "elif aktivaciosfgv == 'sigmoid':\n",
        "  def activation(x):\n",
        "    return 1.0/(1.0 + numpy.exp(-x))\n",
        "  def dactivation(x):\n",
        "    return x*(1.0-x)\n",
        "\n",
        "'''Bővíthetőség miatt maradnak elif-ek'''\n",
        "if algoritmus == 'A':\n",
        "  acti, dacti = activation, dactivation\n",
        "\n",
        "  ''' Zsolti: Ide jöhet majd az adatbázis olvasás '''\n",
        "  #samples = [[[0, 0], [0, 0]], [[0, 1], [0, 1]], [[1, 0], [0, 1]], [[1, 1], [1, 1]]]   # AND / OR\n",
        "  samples = [ [[0, 0], [0, 0, 0]], [[0, 1], [0, 1, 1]], [[1, 0], [0, 1, 1]], [[1, 1], [1, 1, 0]] ]  # AND / OR / XOR\n",
        "\n",
        "  \n",
        "\n",
        "  B = 1\n",
        "  nn = [len(samples[0][0])+B, 4, 3, len(samples[0][1])]\n",
        "  wl = [ numpy.random.random((nn[l+1], nn[l]))*0.8-0.4 for l in range(len(nn)-1)] \n",
        "  delta = [numpy.zeros((nn[l+1])) for l in range(len(nn)-1)]\n",
        "\n",
        "  epoch = 0\n",
        "  sumerr = 1.0\n",
        "  while sumerr>=0.01 and epoch<=10000:\n",
        "    sumerr = 0.0\n",
        "    epoch += 1\n",
        "    for inp, out in samples:\n",
        "        nl = [ numpy.array(inp + [1.0]*B) ]\n",
        "        for l in range(len(nn)-1):\n",
        "            nl.append(acti(numpy.dot(wl[l],nl[l])))\n",
        "        error = out - nl[-1]\n",
        "        for l in reversed(range(len(nn)-1)):\n",
        "            if l == len(nn)-2:\n",
        "                delta[l][:] = error*dacti(nl[-1])\n",
        "            else:\n",
        "                numpy.dot(delta[l+1],wl[l+1], out=delta[l])\n",
        "                delta[l] *= dacti(nl[l+1])\n",
        "            wl[l] += 0.5 * delta[l].reshape((nn[l+1],1))*nl[l].reshape((1,nn[l]))\n",
        "        sumerr += sum(error**2)\n",
        "  print (epoch,sumerr)\n",
        "elif algoritmus == 'B':\n",
        "\n",
        "  acti, dacti = activation, dactivation\n",
        "  #Adatbázist ide - Henrik\n",
        "  #samples = [[[0, 0], [0, 0]], [[0, 1], [0, 1]], [[1, 0], [0, 1]], [[1, 1], [1, 1]]]   # AND és OR\n",
        "  samples = [ [[0, 0], [0, 0, 0]], [[0, 1], [0, 1, 1]], [[1, 0], [0, 1, 1]], [[1, 1], [1, 1, 0]] ]  # AND, OR és XOR\n",
        "\n",
        "  B = 1\n",
        "  nn = [len(samples[0][0])+B, 4, 3, len(samples[0][1])]\n",
        "  wl=[ [ [random.random()*0.8-0.4 for _ in range(nn[l])] for _ in range(nn[l+1])] for l in range(len(nn)-1)] \n",
        "  epoch = 0\n",
        "  sumerr = 1.0\n",
        "  while sumerr>=0.01 and epoch<=10000:\n",
        "      sumerr = 0.0\n",
        "      epoch += 1\n",
        "      for inp, out in samples:\n",
        "          nl = [ inp + [1.0]*B ]\n",
        "          for l in range(len(nn)-1):\n",
        "              nl.append([acti(sum([nl[l][i] * wl[l][j][i] for i in range(nn[l])])) for j in range(nn[l+1])])\n",
        "              \n",
        "          error = [out[j] - nl[-1][j] for j in range(nn[-1])]\n",
        "          delta = [None for _ in range(len(nn)-1)]\n",
        "          for l in reversed(range(len(nn)-1)):\n",
        "              if l == len(nn)-2:\n",
        "                  delta[l] = [error[j] * dacti(nl[-1][j]) for j in range(nn[-1])]\n",
        "              else:\n",
        "                  delta[l] = [sum([delta[l+1][j] * wl[l+1][j][i] for j in range(nn[l+2])])*dacti(nl[l+1][i]) for i in range(nn[l+1])]\n",
        "              \n",
        "              for i in range(nn[l]):\n",
        "                  for j in range(nn[l+1]):\n",
        "                      wl[l][j][i] += 0.5 * delta[l][j] * nl[l][i]\n",
        "\n",
        "          sumerr += sum( [error[j]**2 for j in range(nn[-1])])\n",
        "  print (epoch,sumerr)\n",
        "\n",
        "elif algoritmus == 'C':\n",
        "  acti, dacti = activation, dactivation\n",
        "\n",
        "  w1 = [random.random()*0.4-0.2 for _ in range(3)]\n",
        "  #Adatbázist ide - Henrik\n",
        "  #samples = [[[0.0, 0.0], 0.0], [[0.0, 1.0], 1.0], [[1.0, 0.0], 1.0], [[1.0, 1.0], 1.0]] OR\n",
        "  samples = [[[0.0, 0.0], 0.0], [[0.0, 1.0], 0.0], [[1.0, 0.0], 0.0], [[1.0, 1.0], 1.0]] #AND\n",
        "  #samples = [[[0.0, 0.0], 0.0], [[0.0, 1.0], 1.0], [[1.0, 0.0], 1.0], [[1.0, 1.0], 0.0]] XOR\n",
        "\n",
        "  for _ in range(100):\n",
        "      sumerr = 0.0\n",
        "      for inp, out in samples:\n",
        "          x = inp + [1.0]\n",
        "          y = acti(sum([x[i] * w1[i] for i in range(3)]))\n",
        "          error = (out - y)\n",
        "          delta = error * dacti(y)\n",
        "          for i in range(3):\n",
        "              w1[i] += 0.8 * delta * x[i]\n",
        "          sumerr += (y-out)**2\n",
        "      print (sumerr)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xUjgAUsUb6Q",
        "outputId": "67c9469e-491e-4846-c1fb-b72467860b81"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.091120410005597\n",
            "1.010294233612718\n",
            "0.9477963530579934\n",
            "0.8979572844060801\n",
            "0.8567450369232786\n",
            "0.8214452795871171\n",
            "0.7902792335100293\n",
            "0.7620985886726745\n",
            "0.7361695439675714\n",
            "0.7120259700969791\n",
            "0.689370907830866\n",
            "0.6680112586222571\n",
            "0.6478154618852004\n",
            "0.6286871126021205\n",
            "0.6105493850080451\n",
            "0.5933364327379189\n",
            "0.5769889890315836\n",
            "0.5614522878594332\n",
            "0.546675146041744\n",
            "0.532609562298859\n",
            "0.5192105155404698\n",
            "0.5064358259324958\n",
            "0.494246029796208\n",
            "0.4826042552179922\n",
            "0.4714760968372672\n",
            "0.46082949057682704\n",
            "0.4506345888466128\n",
            "0.44086363638641357\n",
            "0.43149084691109485\n",
            "0.4224922809598191\n",
            "0.4138457256226994\n",
            "0.40553057699799394\n",
            "0.39752772628179367\n",
            "0.3898194503259922\n",
            "0.38238930735689225\n",
            "0.37522203836524415\n",
            "0.3683034744899988\n",
            "0.3616204505433142\n",
            "0.3551607246748353\n",
            "0.34891290405335107\n",
            "0.3428663763534997\n",
            "0.33701124677143607\n",
            "0.3313382802522722\n",
            "0.3258388485893447\n",
            "0.3205048820468386\n",
            "0.3153288251593968\n",
            "0.3103035963720435\n",
            "0.305422551198614\n",
            "0.3006794485950137\n",
            "0.29606842026357094\n",
            "0.29158394262544973\n",
            "0.2872208112187745\n",
            "0.2829741173002645\n",
            "0.2788392264474264\n",
            "0.27481175897649884\n",
            "0.2708875720082668\n",
            "0.2670627430295021\n",
            "0.26333355481217574\n",
            "0.25969648156572595\n",
            "0.2561481762096428\n",
            "0.2526854586644924\n",
            "0.24930530506934978\n",
            "0.2460048378425015\n",
            "0.24278131651030643\n",
            "0.23963212923634328\n",
            "0.23655478498948834\n",
            "0.23354690629544075\n",
            "0.23060622252149404\n",
            "0.2277305636491049\n",
            "0.22491785449309243\n",
            "0.22216610933014969\n",
            "0.2194734269028153\n",
            "0.21683798576817603\n",
            "0.21425803996338458\n",
            "0.2117319149626093\n",
            "0.20925800390231983\n",
            "0.20683476405387924\n",
            "0.20446071352426543\n",
            "0.20213442816744187\n",
            "0.19985453869040323\n",
            "0.19761972793931323\n",
            "0.19542872835238356\n",
            "0.19328031956728398\n",
            "0.19117332617188737\n",
            "0.1891066155880917\n",
            "0.1870790960792958\n",
            "0.1850897148728785\n",
            "0.183137456389727\n",
            "0.18122134057349046\n",
            "0.17934042131282238\n",
            "0.1774937849503919\n",
            "0.1756805488729345\n",
            "0.17389986017704684\n",
            "0.1721508944058321\n",
            "0.17043285435187416\n",
            "0.16874496892234914\n",
            "0.1670864920623965\n",
            "0.16545670173314952\n",
            "0.16385489894109\n",
            "0.16228040681562406\n"
          ]
        }
      ]
    }
  ]
}